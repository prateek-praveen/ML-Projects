{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural_language_processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9hIofdEROda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmlMOxqFRbKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6d764c2e-75b0-40e4-9712-911924eaa7e1"
      },
      "source": [
        "import sys\n",
        "import sklearn\n",
        "\n",
        "print('Pythons {}', format(sys.version))\n",
        "print('NLTK {}', format(nltk.__version__))\n",
        "print('Sklearn {}', format(sklearn.__version__))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pythons {} 3.6.9 (default, Jul 17 2020, 12:50:27) \n",
            "[GCC 8.4.0]\n",
            "NLTK {} 3.2.5\n",
            "Sklearn {} 0.22.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKgpQBT_SH89",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a2fcb058-62e6-4faf-c20f-940ecb780b3b"
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> x\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTrkB6etVqpD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4ddb6c49-ab37-48e8-d978-a2946b681511"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYJvDS5jTtj9",
        "colab_type": "text"
      },
      "source": [
        "### **Corpus -**\n",
        "Body of text, singular. Corpora is the plural of this. Example: A collection of medical journals.\n",
        "\n",
        "### **Lexicon -**\n",
        "Words and their meanings. Example: English dictionary. Consider, however, that various fields will have different lexicons.\n",
        "\n",
        "###**Token -**\n",
        "Each \"entity\" that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is \"tokenized\" into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaxOZ6CyUC3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5137e024-b78a-493f-ff4b-5445aa6f1de5"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize #Tokenize on the basis of sentenses\n",
        "from nltk.tokenize import word_tokenize #Tokenoze on the basis of words\n",
        "\n",
        "text = \"Hello students, how are you doing today? The olympics are inspiring, and Python is awesome. You look nice today.\"\n",
        "\n",
        "print(sent_tokenize(text))\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello students, how are you doing today?', 'The olympics are inspiring, and Python is awesome.', 'You look nice today.']\n",
            "['Hello', 'students', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'olympics', 'are', 'inspiring', ',', 'and', 'Python', 'is', 'awesome', '.', 'You', 'look', 'nice', 'today', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm5lOikvVWxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "09d7c22a-a861-4e59-b4dd-f8c519f50e60"
      },
      "source": [
        "# Removing stop words --> useless data\n",
        "from nltk.corpus import stopwords\n",
        "print(set(stopwords.words('english')))  # We can change the language as well"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'was', 'hers', 'hadn', 'doesn', \"isn't\", \"didn't\", 'not', 'other', 'our', 'wasn', 'the', 'when', 'she', 'yourself', 'of', 'can', 'few', 'have', 'above', 'ma', 'on', 'did', 'themselves', 'until', \"doesn't\", 'into', 'been', 'to', 'it', 'over', 'no', 'him', 'then', 'a', 'my', 'what', 'or', 'during', 'ourselves', 'nor', 'be', 'your', 'myself', 'before', 'were', 'mightn', 'too', \"wasn't\", 'doing', 'further', 'out', 'had', \"mightn't\", 'only', 'he', 'an', 'couldn', 'them', 'has', \"shouldn't\", 'i', 'those', 'with', 'own', 'they', 'most', 'her', 'which', 'such', 're', 'shan', 'does', 'shouldn', 'all', 'do', 'by', \"weren't\", 'off', 'very', \"needn't\", 'mustn', 'haven', 'how', \"hasn't\", 'ain', \"won't\", 'you', 'wouldn', 'will', 's', 'some', \"aren't\", 'about', 'who', 've', 'o', 'having', 'again', 'his', 't', \"you've\", 'after', 'up', 'in', 'just', \"should've\", 'd', 'me', 'below', \"haven't\", 'we', 'against', 'now', 'both', \"couldn't\", \"she's\", 'isn', 'and', \"don't\", 'their', 'because', 'down', 'didn', 'under', 'from', 'that', 'itself', 'at', 'don', 'needn', 'there', 'are', 'here', 'is', \"it's\", 'yours', 'as', 'aren', 'if', 'each', 'whom', 'hasn', 'won', 'these', \"you're\", \"that'll\", \"mustn't\", 'being', 'so', \"you'll\", 'weren', 'where', 'for', 'through', 'am', 'y', 'once', \"you'd\", 'while', 'll', 'theirs', \"wouldn't\", \"hadn't\", \"shan't\", 'ours', 'why', 'between', 'same', 'but', 'any', 'its', 'yourselves', 'than', 'should', 'm', 'herself', 'more', 'himself', 'this'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhSZWI7gXtBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d0776477-945b-4822-e5a5-d7bab946d8f5"
      },
      "source": [
        "example_sent = \"This is some sample text, showing off the stop words filtration.\"\n",
        "\n",
        "stop_word = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "  if w not in stop_word:\n",
        "      filtered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'is', 'some', 'sample', 'text', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'text', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNnr6WTUcyG4",
        "colab_type": "text"
      },
      "source": [
        "## **Stemming Words with NLTK:**\n",
        "Stemming, which attempts to normalize sentences, is another preprocessing step that we can perform. In the english language, different variations of words and sentences often having the same meaning. Stemming is a way to account for these variations; furthermore, it will help us shorten the sentences and shorten our lookup. For example, consider the following sentence:\n",
        "\n",
        "* I was taking a ride on my horse.\n",
        "*I was riding my horse.\n",
        "\n",
        "These sentences mean the same thing, as noted by the same tense (-ing) in each sentence; however, that isn't intuitively understood by the computer. To account for all the variations of words in the english language, we can use the Porter stemmer, which has been around since 1979."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPcyZ8UCc_Pd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "aabf535c-0473-4cd9-b648-458dec5f2957"
      },
      "source": [
        "# Stemming words with NLTk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "example_words = [\"ride\", \"riding\", \"rider\", \"rides\"]\n",
        "\n",
        "for w in example_words:\n",
        "  print(ps.stem(w))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ride\n",
            "ride\n",
            "rider\n",
            "ride\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK0RUagPeS_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "ebda06ba-25de-4ce6-8aea-a00ebbf5b1aa"
      },
      "source": [
        "# Stemming an entire sentence!\n",
        "\n",
        "new_text = \"When riders are riding their horses, they often think of how cowboys rode horses.\"\n",
        "\n",
        "words = word_tokenize(new_text)\n",
        "\n",
        "for w in words:\n",
        "    print(ps.stem(w))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "when\n",
            "rider\n",
            "are\n",
            "ride\n",
            "their\n",
            "hors\n",
            ",\n",
            "they\n",
            "often\n",
            "think\n",
            "of\n",
            "how\n",
            "cowboy\n",
            "rode\n",
            "hors\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnOaeLxffnNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d1e34026-79c9-4afe-fa20-b5a1bd54a42d"
      },
      "source": [
        "nltk.download('udhr')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/udhr.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8OkOBe_fOLC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75e48eb8-2881-4b42-cce1-579f383fc56d"
      },
      "source": [
        "# We can use documents from the nltk.corpus.  As an example, lets load the universal declaration of human rights.\n",
        "\n",
        "from nltk.corpus import udhr\n",
        "print(udhr.raw('English-Latin1'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Universal Declaration of Human Rights\n",
            "Preamble\n",
            "Whereas recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world, \n",
            "\n",
            "Whereas disregard and contempt for human rights have resulted in barbarous acts which have outraged the conscience of mankind, and the advent of a world in which human beings shall enjoy freedom of speech and belief and freedom from fear and want has been proclaimed as the highest aspiration of the common people, \n",
            "\n",
            "Whereas it is essential, if man is not to be compelled to have recourse, as a last resort, to rebellion against tyranny and oppression, that human rights should be protected by the rule of law, \n",
            "\n",
            "Whereas it is essential to promote the development of friendly relations between nations, \n",
            "\n",
            "Whereas the peoples of the United Nations have in the Charter reaffirmed their faith in fundamental human rights, in the dignity and worth of the human person and in the equal rights of men and women and have determined to promote social progress and better standards of life in larger freedom, \n",
            "\n",
            "Whereas Member States have pledged themselves to achieve, in cooperation with the United Nations, the promotion of universal respect for and observance of human rights and fundamental freedoms, \n",
            "\n",
            "Whereas a common understanding of these rights and freedoms is of the greatest importance for the full realization of this pledge, \n",
            "\n",
            "Now, therefore, \n",
            "\n",
            "The General Assembly, \n",
            "\n",
            "Proclaims this Universal Declaration of Human Rights as a common standard of achievement for all peoples and all nations, to the end that every individual and every organ of society, keeping this Declaration constantly in mind, shall strive by teaching and education to promote respect for these rights and freedoms and by progressive measures, national and international, to secure their universal and effective recognition and observance, both among the peoples of Member States themselves and among the peoples of territories under their jurisdiction. \n",
            "\n",
            "Article 1 \n",
            "All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. \n",
            "\n",
            "Article 2 \n",
            "Everyone is entitled to all the rights and freedoms set forth in this Declaration, without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status. \n",
            "\n",
            "Furthermore, no distinction shall be made on the basis of the political, jurisdictional or international status of the country or territory to which a person belongs, whether it be independent, trust, non-self-governing or under any other limitation of sovereignty. \n",
            "\n",
            "Article 3 \n",
            "Everyone has the right to life, liberty and security of person. \n",
            "\n",
            "Article 4 \n",
            "No one shall be held in slavery or servitude; slavery and the slave trade shall be prohibited in all their forms. \n",
            "\n",
            "Article 5 \n",
            "No one shall be subjected to torture or to cruel, inhuman or degrading treatment or punishment. \n",
            "\n",
            "Article 6 \n",
            "Everyone has the right to recognition everywhere as a person before the law. \n",
            "\n",
            "Article 7 \n",
            "All are equal before the law and are entitled without any discrimination to equal protection of the law. All are entitled to equal protection against any discrimination in violation of this Declaration and against any incitement to such discrimination. \n",
            "\n",
            "Article 8 \n",
            "Everyone has the right to an effective remedy by the competent national tribunals for acts violating the fundamental rights granted him by the constitution or by law. \n",
            "\n",
            "Article 9 \n",
            "No one shall be subjected to arbitrary arrest, detention or exile. \n",
            "\n",
            "Article 10 \n",
            "Everyone is entitled in full equality to a fair and public hearing by an independent and impartial tribunal, in the determination of his rights and obligations and of any criminal charge against him. \n",
            "\n",
            "Article 11 \n",
            "Everyone charged with a penal offence has the right to be presumed innocent until proved guilty according to law in a public trial at which he has had all the guarantees necessary for his defence. \n",
            "No one shall be held guilty of any penal offence on account of any act or omission which did not constitute a penal offence, under national or international law, at the time when it was committed. Nor shall a heavier penalty be imposed than the one that was applicable at the time the penal offence was committed. \n",
            "Article 12 \n",
            "No one shall be subjected to arbitrary interference with his privacy, family, home or correspondence, nor to attacks upon his honour and reputation. Everyone has the right to the protection of the law against such interference or attacks. \n",
            "\n",
            "Article 13 \n",
            "Everyone has the right to freedom of movement and residence within the borders of each State. \n",
            "Everyone has the right to leave any country, including his own, and to return to his country. \n",
            "Article 14 \n",
            "Everyone has the right to seek and to enjoy in other countries asylum from persecution. \n",
            "This right may not be invoked in the case of prosecutions genuinely arising from non-political crimes or from acts contrary to the purposes and principles of the United Nations. \n",
            "Article 15 \n",
            "Everyone has the right to a nationality. \n",
            "No one shall be arbitrarily deprived of his nationality nor denied the right to change his nationality. \n",
            "Article 16 \n",
            "Men and women of full age, without any limitation due to race, nationality or religion, have the right to marry and to found a family. They are entitled to equal rights as to marriage, during marriage and at its dissolution. \n",
            "Marriage shall be entered into only with the free and full consent of the intending spouses. \n",
            "The family is the natural and fundamental group unit of society and is entitled to protection by society and the State. \n",
            "Article 17 \n",
            "Everyone has the right to own property alone as well as in association with others. \n",
            "No one shall be arbitrarily deprived of his property. \n",
            "Article 18 \n",
            "Everyone has the right to freedom of thought, conscience and religion; this right includes freedom to change his religion or belief, and freedom, either alone or in community with others and in public or private, to manifest his religion or belief in teaching, practice, worship and observance. \n",
            "\n",
            "Article 19 \n",
            "Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers. \n",
            "\n",
            "Article 20 \n",
            "Everyone has the right to freedom of peaceful assembly and association. \n",
            "No one may be compelled to belong to an association. \n",
            "Article 21 \n",
            "Everyone has the right to take part in the government of his country, directly or through freely chosen representatives. \n",
            "Everyone has the right to equal access to public service in his country. \n",
            "The will of the people shall be the basis of the authority of government; this will shall be expressed in periodic and genuine elections which shall be by universal and equal suffrage and shall be held by secret vote or by equivalent free voting procedures. \n",
            "Article 22 \n",
            "Everyone, as a member of society, has the right to social security and is entitled to realization, through national effort and international co-operation and in accordance with the organization and resources of each State, of the economic, social and cultural rights indispensable for his dignity and the free development of his personality. \n",
            "\n",
            "Article 23 \n",
            "Everyone has the right to work, to free choice of employment, to just and favourable conditions of work and to protection against unemployment. \n",
            "Everyone, without any discrimination, has the right to equal pay for equal work. \n",
            "Everyone who works has the right to just and favourable remuneration ensuring for himself and his family an existence worthy of human dignity, and supplemented, if necessary, by other means of social protection. \n",
            "Everyone has the right to form and to join trade unions for the protection of his interests. \n",
            "Article 24 \n",
            "Everyone has the right to rest and leisure, including reasonable limitation of working hours and periodic holidays with pay. \n",
            "\n",
            "Article 25 \n",
            "Everyone has the right to a standard of living adequate for the health and well-being of himself and of his family, including food, clothing, housing and medical care and necessary social services, and the right to security in the event of unemployment, sickness, disability, widowhood, old age or other lack of livelihood in circumstances beyond his control. \n",
            "Motherhood and childhood are entitled to special care and assistance. All children, whether born in or out of wedlock, shall enjoy the same social protection. \n",
            "Article 26 \n",
            "Everyone has the right to education. Education shall be free, at least in the elementary and fundamental stages. Elementary education shall be compulsory. Technical and professional education shall be made generally available and higher education shall be equally accessible to all on the basis of merit. \n",
            "Education shall be directed to the full development of the human personality and to the strengthening of respect for human rights and fundamental freedoms. It shall promote understanding, tolerance and friendship among all nations, racial or religious groups, and shall further the activities of the United Nations for the maintenance of peace. \n",
            "Parents have a prior right to choose the kind of education that shall be given to their children. \n",
            "Article 27 \n",
            "Everyone has the right freely to participate in the cultural life of the community, to enjoy the arts and to share in scientific advancement and its benefits. \n",
            "Everyone has the right to the protection of the moral and material interests resulting from any scientific, literary or artistic production of which he is the author. \n",
            "Article 28 \n",
            "Everyone is entitled to a social and international order in which the rights and freedoms set forth in this Declaration can be fully realized. \n",
            "\n",
            "Article 29 \n",
            "Everyone has duties to the community in which alone th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7HUyJNkgUJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "388701c5-238a-4f57-dfdf-6426d8e8ea5f"
      },
      "source": [
        "nltk.download('state_union')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXMUyNyvgj_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets import some sample and training text - George Bush's 2005 and 2006 state of the union addresses. \n",
        "\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "# The PunktSentenceTokenizer is an unsupervised trainable model. \n",
        "# This means it can be trained on unlabeled data, aka text that is not split into sentences.\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "#print(train_text)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glnqK28cg0x1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now that we have some text, we can train the PunktSentenceTokenizer\n",
        "\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ehutkQalmLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets tokenize the sample_text using our trained tokenizer\n",
        "\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "# print(tokenized)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvjM0HXgnIy8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "93e29b0a-c5d1-46f2-8309-8fae9c2a0ef2"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_ZysFxfl5Wx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "751fc404-1121-4ec0-e509-d0fdbb986b6c"
      },
      "source": [
        "# This function will tag each tokenized word with a part of speech\n",
        "\n",
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized[:5]: # Doing for the first five sentences\n",
        "        words = nltk.word_tokenize(i)\n",
        "        tagged = nltk.pos_tag(words)\n",
        "        print(tagged)\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "\n",
        "# The output is a list of tuples - the word with it's part of speech\n",
        "process_content()          "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
            "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
            "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
            "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
            "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbGo5VArnPmB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "294c8d43-53f9-489a-b2a3-63df30a57b2a"
      },
      "source": [
        "# Understanding what does that labels mean\n",
        "nltk.download('tagsets')\n",
        "\n",
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI4z2n4FpnXC",
        "colab_type": "text"
      },
      "source": [
        "### **Chunking with NLTK**\n",
        "Now that each word has been tagged with a part of speech, we can move onto chunking: grouping the words into meaningful clusters. The main goal of chunking is to group words into \"noun phrases\", which is a noun with any associated verbs, adjectives, or adverbs.\n",
        "\n",
        "The part of speech tags that were generated in the previous step will be combined with regular expressions, such as the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOm-iHBVpuJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "+ = match 1 or more\n",
        "? = match 0 or 1 repetitions.\n",
        "* = match 0 or MORE repetitions\t  \n",
        ". = Any character except a new line\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-SGYfY4p62m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            \n",
        "            # combine the part-of-speech tag with a regular expression\n",
        "            \n",
        "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            \n",
        "            # draw the chunks with nltk\n",
        "            # chunked.draw()     \n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "        \n",
        "process_content()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTmm0n2Zslti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "\n",
        "This line, broken down:\n",
        "\n",
        "\n",
        "<RB.?>* = \"0 or more of any tense of adverb,\" followed by: \n",
        "\n",
        "<VB.?>* = \"0 or more of any tense of verb,\" followed by: \n",
        "\n",
        "<NNP>+ = \"One or more proper nouns,\" followed by \n",
        "\n",
        "<NN>? = \"zero or one singular noun.\" \n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF9Yv_TWsthv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "abaf31be-7370-4aff-d270-19de99f4c17b"
      },
      "source": [
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized[:50]:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            \n",
        "            # combine the part-of-speech tag with a regular expression\n",
        "            \n",
        "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\" #Chunking all the noun phrases\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            \n",
        "            # print(chunked)\n",
        "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
        "                print(subtree) \n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "        \n",
        "process_content()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
            "(Chunk ADDRESS/NNP)\n",
            "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
            "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
            "(Chunk THE/NNP UNION/NNP January/NNP)\n",
            "(Chunk THE/NNP PRESIDENT/NNP)\n",
            "(Chunk Thank/NNP)\n",
            "(Chunk Mr./NNP Speaker/NNP)\n",
            "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
            "(Chunk Congress/NNP)\n",
            "(Chunk Supreme/NNP Court/NNP)\n",
            "(Chunk called/VBD America/NNP)\n",
            "(Chunk Coretta/NNP Scott/NNP King/NNP)\n",
            "(Chunk Applause/NNP)\n",
            "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
            "(Chunk State/NNP)\n",
            "(Chunk Union/NNP Address/NNP)\n",
            "(Chunk Capitol/NNP)\n",
            "(Chunk Tuesday/NNP)\n",
            "(Chunk Jan/NNP)\n",
            "(Chunk White/NNP House/NNP photo/NN)\n",
            "(Chunk Eric/NNP DraperEvery/NNP time/NN)\n",
            "(Chunk Capitol/NNP dome/NN)\n",
            "(Chunk have/VBP served/VBN America/NNP)\n",
            "(Chunk Tonight/NNP)\n",
            "(Chunk Union/NNP)\n",
            "(Chunk Applause/NNP)\n",
            "(Chunk United/NNP)\n",
            "(Chunk America/NNP)\n",
            "(Chunk Applause/NNP)\n",
            "(Chunk America/NNP)\n",
            "(Chunk September/NNP)\n",
            "(Chunk Dictatorships/NNP shelter/NN)\n",
            "(Chunk Applause/NNP)\n",
            "(Chunk Afghanistan/NNP)\n",
            "(Chunk Iraqis/NNP)\n",
            "(Chunk Lebanon/NNP)\n",
            "(Chunk Egypt/NNP)\n",
            "(Chunk Syria/NNP)\n",
            "(Chunk Burma/NNP)\n",
            "(Chunk Zimbabwe/NNP)\n",
            "(Chunk North/NNP Korea/NNP)\n",
            "(Chunk Iran/NNP)\n",
            "(Chunk Applause/NNP)\n",
            "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
            "(Chunk Union/NNP Address/NNP)\n",
            "(Chunk Capitol/NNP)\n",
            "(Chunk Tuesday/NNP)\n",
            "(Chunk Jan/NNP)\n",
            "(Chunk White/NNP House/NNP photo/NN)\n",
            "(Chunk Eric/NNP Draper/NNP No/NNP one/NN)\n",
            "(Chunk Islam/NNP)\n",
            "(Chunk Laden/NNP)\n",
            "(Chunk Middle/NNP East/NNP)\n",
            "(Chunk Iraq/NNP)\n",
            "(Chunk America/NNP)\n",
            "(Chunk Beslan/NNP)\n",
            "(Chunk London/NNP)\n",
            "(Chunk Earth/NNP)\n",
            "(Chunk Applause/NNP)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rJLKLkxvt7_",
        "colab_type": "text"
      },
      "source": [
        "### ***Chinking with NLTK***\n",
        "Sometimes there are words in the chunks that we don't won't, we can remove them using a process called chinking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbH-6UPLvzLq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc95e6d4-3f4b-4d90-c817-e882512623b9"
      },
      "source": [
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized[:20]:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            \n",
        "            # The main difference here is the }{, vs. the {}. This means we're removing \n",
        "            # from the chink one or more verbs, prepositions, determiners, or the word 'to'.\n",
        "\n",
        "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
        "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
        "\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            \n",
        "            # print(chunked)\n",
        "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
        "                print(subtree)\n",
        "\n",
        "            # chunked.draw()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "        \n",
        "process_content()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP 'S/POS ADDRESS/NNP)\n",
            "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
            "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
            "(Chunk\n",
            "  THE/NNP\n",
            "  UNION/NNP\n",
            "  January/NNP\n",
            "  31/CD\n",
            "  ,/,\n",
            "  2006/CD\n",
            "  THE/NNP\n",
            "  PRESIDENT/NNP\n",
            "  :/:\n",
            "  Thank/NNP\n",
            "  you/PRP)\n",
            "(Chunk ./.)\n",
            "(Chunk\n",
            "  Mr./NNP\n",
            "  Speaker/NNP\n",
            "  ,/,\n",
            "  Vice/NNP\n",
            "  President/NNP\n",
            "  Cheney/NNP\n",
            "  ,/,\n",
            "  members/NNS)\n",
            "(Chunk Congress/NNP ,/, members/NNS)\n",
            "(Chunk\n",
            "  Supreme/NNP\n",
            "  Court/NNP\n",
            "  and/CC\n",
            "  diplomatic/JJ\n",
            "  corps/NN\n",
            "  ,/,\n",
            "  distinguished/JJ\n",
            "  guests/NNS\n",
            "  ,/,\n",
            "  and/CC\n",
            "  fellow/JJ\n",
            "  citizens/NNS\n",
            "  :/:)\n",
            "(Chunk our/PRP$ nation/NN)\n",
            "(Chunk ,/, graceful/JJ ,/, courageous/JJ woman/NN who/WP)\n",
            "(Chunk America/NNP)\n",
            "(Chunk its/PRP$ founding/NN ideals/NNS and/CC)\n",
            "(Chunk noble/JJ dream/NN ./.)\n",
            "(Chunk Tonight/NN we/PRP)\n",
            "(Chunk hope/NN)\n",
            "(Chunk glad/JJ reunion/NN)\n",
            "(Chunk husband/NN who/WP)\n",
            "(Chunk so/RB long/RB ago/RB ,/, and/CC we/PRP)\n",
            "(Chunk grateful/JJ)\n",
            "(Chunk good/JJ life/NN)\n",
            "(Chunk Coretta/NNP Scott/NNP King/NNP ./.)\n",
            "(Chunk (/( Applause/NNP ./. )/))\n",
            "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
            "(Chunk his/PRP$ State/NNP)\n",
            "(Chunk Union/NNP Address/NNP)\n",
            "(Chunk Capitol/NNP ,/, Tuesday/NNP ,/, Jan/NNP ./.)\n",
            "(Chunk 31/CD ,/, 2006/CD ./.)\n",
            "(Chunk White/NNP House/NNP photo/NN)\n",
            "(Chunk Eric/NNP DraperEvery/NNP time/NN I/PRP)\n",
            "(Chunk invited/JJ)\n",
            "(Chunk rostrum/NN ,/, I/PRP)\n",
            "(Chunk privilege/NN ,/, and/CC mindful/NN)\n",
            "(Chunk history/NN we/PRP)\n",
            "(Chunk together/RB ./.)\n",
            "(Chunk We/PRP)\n",
            "(Chunk Capitol/NNP dome/NN)\n",
            "(Chunk moments/NNS)\n",
            "(Chunk national/JJ mourning/NN and/CC national/JJ achievement/NN ./.)\n",
            "(Chunk We/PRP)\n",
            "(Chunk America/NNP)\n",
            "(Chunk one/CD)\n",
            "(Chunk most/RBS consequential/JJ periods/NNS)\n",
            "(Chunk our/PRP$ history/NN --/: and/CC it/PRP)\n",
            "(Chunk my/PRP$ honor/NN)\n",
            "(Chunk you/PRP ./.)\n",
            "(Chunk system/NN)\n",
            "(Chunk\n",
            "  two/CD\n",
            "  parties/NNS\n",
            "  ,/,\n",
            "  two/CD\n",
            "  chambers/NNS\n",
            "  ,/,\n",
            "  and/CC\n",
            "  two/CD\n",
            "  elected/JJ\n",
            "  branches/NNS\n",
            "  ,/,\n",
            "  there/EX\n",
            "  will/MD\n",
            "  always/RB)\n",
            "(Chunk differences/NNS and/CC debate/NN ./.)\n",
            "(Chunk But/CC even/RB tough/JJ debates/NNS can/MD)\n",
            "(Chunk\n",
            "  civil/JJ\n",
            "  tone/NN\n",
            "  ,/,\n",
            "  and/CC\n",
            "  our/PRP$\n",
            "  differences/NNS\n",
            "  can/MD\n",
            "  not/RB)\n",
            "(Chunk anger/NN ./.)\n",
            "(Chunk great/JJ issues/NNS)\n",
            "(Chunk us/PRP ,/, we/PRP must/MD)\n",
            "(Chunk spirit/NN)\n",
            "(Chunk goodwill/NN and/CC respect/NN)\n",
            "(Chunk one/CD)\n",
            "(Chunk --/: and/CC I/PRP will/MD)\n",
            "(Chunk my/PRP$ part/NN ./.)\n",
            "(Chunk Tonight/NNP)\n",
            "(Chunk state/NN)\n",
            "(Chunk our/PRP$ Union/NNP)\n",
            "(Chunk strong/JJ --/: and/CC together/RB we/PRP will/MD)\n",
            "(Chunk it/PRP stronger/JJR ./.)\n",
            "(Chunk (/( Applause/NNP ./. )/))\n",
            "(Chunk decisive/JJ year/NN ,/, you/PRP and/CC I/PRP will/MD)\n",
            "(Chunk choices/NNS that/WDT)\n",
            "(Chunk future/NN and/CC)\n",
            "(Chunk character/NN)\n",
            "(Chunk our/PRP$ country/NN ./.)\n",
            "(Chunk We/PRP will/MD)\n",
            "(Chunk confidently/RB)\n",
            "(Chunk enemies/NNS)\n",
            "(Chunk freedom/NN --/: or/CC retreat/NN)\n",
            "(Chunk our/PRP$ duties/NNS)\n",
            "(Chunk hope/NN)\n",
            "(Chunk easier/JJR life/NN ./.)\n",
            "(Chunk We/PRP will/MD)\n",
            "(Chunk our/PRP$ prosperity/NN)\n",
            "(Chunk world/NN economy/NN --/: or/CC)\n",
            "(Chunk ourselves/PRP off/RP)\n",
            "(Chunk trade/NN and/CC opportunity/NN ./.)\n",
            "(Chunk complex/JJ and/CC challenging/JJ time/NN ,/,)\n",
            "(Chunk road/NN)\n",
            "(Chunk isolationism/NN and/CC protectionism/NN may/MD)\n",
            "(Chunk broad/JJ and/CC inviting/NN --/: yet/CC it/PRP)\n",
            "(Chunk danger/NN and/CC decline/NN ./.)\n",
            "(Chunk only/JJ way/NN)\n",
            "(Chunk our/PRP$ people/NNS ,/,)\n",
            "(Chunk only/JJ way/NN)\n",
            "(Chunk peace/NN ,/,)\n",
            "(Chunk only/JJ way/NN)\n",
            "(Chunk our/PRP$ destiny/NN)\n",
            "(Chunk our/PRP$ leadership/NN --/:)\n",
            "(Chunk United/NNP States/NNPS)\n",
            "(Chunk America/NNP will/MD)\n",
            "(Chunk ./.)\n",
            "(Chunk (/( Applause/NNP ./. )/))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m2IspOaxT7q",
        "colab_type": "text"
      },
      "source": [
        "### **Named Entity Recognition with NLTK**\n",
        "One of the most common forms of chunking in natural language processing is called \"Named Entity Recognition.\" NLTK is able to identify people, places, things, locations, monetary figures, and more.\n",
        "\n",
        "There are two major options with NLTK's named entity recognition: either recognize all named entities, or recognize named entities as their respective type, like people, places, locations, etc.\n",
        "\n",
        "Here, with the option of binary = True, this means either something is a named entity, or not. There will be no further detail.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3L9nipMxY1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "62c63682-dc05-4a0b-a7a1-6c0e7fc8a1b5"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized[5:]:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
        "            # namedEnt.draw()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "        \n",
        "process_content()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "name 'tokenized' is not defined\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}